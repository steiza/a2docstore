#!/usr/bin/env python

import base64
from datetime import datetime, time
import json
import logging
import mimetypes
import os
import shutil
import sys
from urllib.parse import quote, unquote, unquote_plus, urlencode

import bleach
import boto3
from dateutil import tz
from feedgen.feed import FeedGenerator
import markdown
from sqlalchemy import Column, create_engine, desc, Date, func, Integer, or_, String
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from tornado.httpserver import HTTPServer
from tornado.ioloop import IOLoop
from tornado.web import Application, RequestHandler, StaticFileHandler, stream_request_body
import yaml


BLEACH_ALLOWED_TAGS = bleach.ALLOWED_TAGS + ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre']


class IndexHandler(RequestHandler):

    def initialize(self, region, google_analytics_id, SessionMaker):
        self.__region = region
        self.__google_analytics_id = google_analytics_id
        self.__SessionMaker = SessionMaker

    def get(self):
        authorized = self.get_secure_cookie('authorized')

        session = self.__SessionMaker()
        recent_docs = session.query(DocModel).order_by(desc('date_uploaded')).limit(10)
        session.close()

        notification = self.get_cookie('notification')

        if notification:
            notification = unquote(notification)
            self.clear_cookie('notification')

        self.render(
            'index.html', region=self.__region,
            google_analytics_id=self.__google_analytics_id,
            authorized=authorized, recent_docs=recent_docs,
            notification=notification, urlencode=urlencode
            )


class AddHandler(RequestHandler):

    def initialize(
        self, region, google_analytics_id, SessionMaker, stored_docs_path,
        s3_bucket_name=None):

        self.__region = region
        self.__google_analytics_id = google_analytics_id
        self.__SessionMaker = SessionMaker
        self.__stored_docs_path = stored_docs_path
        self.__s3_bucket_name = s3_bucket_name

    def get(self):
        authorized = self.get_secure_cookie('authorized')

        session = self.__SessionMaker()

        org_results = session.query(DocModel.source_org).group_by(
                DocModel.source_org).order_by(DocModel.source_org).all()

        org_names = [each[0] for each in org_results]

        session.close()

        self.render(
            'add.html', region=self.__region, org_names=org_names,
            google_analytics_id=self.__google_analytics_id,
            authorized=authorized
            )

    def post(self):
        new_doc = DocModel()

        # Get required arguments
        for each_property in ['doc_title', 'doc_description', 'source_org']:
            value = self.get_argument(each_property)
            setattr(new_doc, each_property, value)

        # Make sure file contents are there
        file_array = self.request.files.get('file', None)

        if not file_array or len(file_array) == 0:
            self.set_status(400)
            self.write('Request missing file upload')
            return

        # Get optional fields
        tracking_number = self.get_argument('tracking_number', default=None)
        new_doc.tracking_number = tracking_number

        date_requested = self.get_argument('date_requested', default=None)
        if date_requested:
            date_requested = datetime.strptime(date_requested, '%m/%d/%Y').date()
        else:
            date_requested = None

        new_doc.date_requested = date_requested

        date_received = self.get_argument('date_received', default=None)
        if date_received:
            date_received = datetime.strptime(date_received, '%m/%d/%Y').date()
        else:
            date_received = None

        new_doc.date_received = date_received

        new_doc.uploader_name = self.get_argument(
            'uploader_name') or 'anonymous'

        new_doc.uploader_email = self.get_argument(
            'uploader_email') or 'anonymous@example.com'

        # Set metadata
        new_doc.date_uploaded = datetime.now().date()

        # Save document in sqlite to get id number
        session = self.__SessionMaker()
        session.add(new_doc)
        session.commit()

        document_id = new_doc.id
        session.close()

        # If S3 is configured, save file there
        if self.__s3_bucket_name:
            s3_client = boto3.client('s3')

            for each_file in file_array:
                s3_key = 'file/{}/{}'.format(document_id, each_file['filename'])
                s3_client.put_object(
                    Body=each_file['body'],
                    Bucket=self.__s3_bucket_name,
                    Key=s3_key,
                )

        # Otherwise, save to disk
        else:
            # Make the directory
            directory = os.path.join(self.__stored_docs_path, str(document_id))
            os.mkdir(directory)

            # Write out the files to disk
            for each_file in file_array:
                file_data = each_file['body']
                filename = each_file['filename']

                # Use id number to write to disk
                file_path = os.path.join(directory, filename)

                fd = open(file_path, 'wb')
                fd.write(file_data)
                fd.close()

        self.set_cookie('notification', quote('Document added; thanks!'))
        self.redirect('/')


class SearchHandler(RequestHandler):

    def initialize(self, region, google_analytics_id, SessionMaker):
        self.__region = region
        self.__google_analytics_id = google_analytics_id
        self.__SessionMaker = SessionMaker

    def get(self):
        authorized = self.get_secure_cookie('authorized')

        query_arg = self.get_argument('query', default=None)
        offset = self.get_argument('offset', default=None)
        search_field = self.get_argument('search_field', default=None)

        if not offset:
            offset = 0
        else:
            offset = int(offset)

        session = self.__SessionMaker()
        query = session.query(DocModel)

        if query_arg:
            if search_field:
                search_attr = getattr(DocModel, search_field)
                query = query.filter(search_attr.like('%%%s%%' % query_arg))

            else:
                # Search all fields
                query = query.filter(or_(
                    DocModel.doc_title.like('%%%s%%' % query_arg),
                    DocModel.doc_description.like('%%%s%%' % query_arg),
                    DocModel.source_org.like('%%%s%%' % query_arg),
                    DocModel.uploader_name.like('%%%s%%' % query_arg)
                    ))

        else:
            query_arg = ''

        query = query.order_by(desc(DocModel.date_uploaded))

        count = query.count()

        matching_docs = query.offset(offset).limit(20).all()

        session.close()

        self.render(
            'search.html', region=self.__region,
            google_analytics_id=self.__google_analytics_id,
            authorized=authorized, query=query_arg, offset=offset,
            search_field=search_field, count=count,
            matching_docs=matching_docs, markdown=markdown, bleach=bleach,
            urlencode=urlencode
            )


class LegacyHandler(RequestHandler):

    def get(self, document_id):
        # Redirect with 'Moved Permanently'
        self.set_status(301)
        self.redirect('/view/{0}'.format(document_id))


class LegacyFileHandler(RequestHandler):

    def initialize(self, SessionMaker):
        self.__SessionMaker = SessionMaker

    def get(self, document_id, file_id):
        session = self.__SessionMaker()

        doc = session.query(LegacyFile).filter(
            LegacyFile.doc_id == document_id).filter(
            LegacyFile.id == file_id).one()

        # Redirect with 'Moved Permanently'
        self.set_status(301)
        self.redirect('/view/{0}/{1}'.format(document_id, doc.filename))


class ViewHandler(RequestHandler):

    def initialize(
            self, region, google_analytics_id, SessionMaker, stored_docs_path,
            s3_bucket_name=None):

        self.__region = region
        self.__google_analytics_id = google_analytics_id
        self.__SessionMaker = SessionMaker
        self.__stored_docs_path = stored_docs_path
        self.__s3_bucket_name = s3_bucket_name

    def get(self, document_id, filename=None):
        authorized = self.get_secure_cookie('authorized')

        session = self.__SessionMaker()
        doc = session.query(DocModel).filter(DocModel.id == document_id).one()
        session.close()

        allowed_tags = bleach.ALLOWED_TAGS + ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre']

        files = []

        # If S3 is configured, check there first
        if self.__s3_bucket_name:
            s3_client = boto3.client('s3')
            objects = s3_client.list_objects(
                Bucket=self.__s3_bucket_name,
                Prefix='file/{}/'.format(document_id),
            )

            for each_object in objects.get('Contents', []):
                files.append(each_object['Key'].split('/')[-1])

        # If we didn't find anything, check on-disk
        if not files:
            doc_folder = os.path.join(self.__stored_docs_path, str(document_id))
            files.extend(os.listdir(doc_folder))

        self.render(
            'view.html', region=self.__region,
            google_analytics_id=self.__google_analytics_id,
            authorized=authorized, doc=doc, filename=filename, bleach=bleach,
            markdown=markdown, allowed_tags=allowed_tags, files=files,
            urlencode=urlencode, xsrf_token=self.xsrf_token,
            )


class DownloadHandler(RequestHandler):

    def initialize(self, stored_docs_path, s3_bucket_name=None):
        self.__stored_docs_path = stored_docs_path
        self.__s3_bucket_name = s3_bucket_name

    def get(self, doc_id, filename):
        filename = unquote_plus(filename)

        # If S3 is configured, attempt to get file from there
        if self.__s3_bucket_name:
            s3_key = 'file/{}/{}'.format(doc_id, filename)
            s3_client = boto3.client('s3')
            objects = s3_client.list_objects(
                Bucket=self.__s3_bucket_name,
                Prefix=s3_key,
            )

            if objects.get('Contents'):
                presigned_url = s3_client.generate_presigned_url(
                    'get_object',
                    Params={'Bucket': self.__s3_bucket_name, 'Key': s3_key},
                )
                self.redirect(presigned_url)
                return

        # Object was not in S3, so try on-disk
        file_path = os.path.join(self.__stored_docs_path, str(doc_id), filename)

        if not os.path.exists(file_path):
            self.set_status(404)
            self.write('File not found')

        file_extension = '.' + filename.split('.')[-1]

        content_type = mimetypes.types_map.get(file_extension)

        if content_type:
            self.set_header('Content-Type', content_type)
        else:
            self.set_header('Content-Type', 'application/octet-stream')
            self.set_header(
                'Content-Disposition', 'attachment; filename="{0}"'.format(
                    filename
                )
            )

        fd = open(file_path, 'rb')

        while True:
            chunk = fd.read(1000000)

            if chunk:
                self.write(chunk)

            else:
                fd.close()
                break


class EditHandler(RequestHandler):

    def initialize(self, region, google_analytics_id, SessionMaker):
        self.__region = region
        self.__google_analytics_id = google_analytics_id
        self.__SessionMaker = SessionMaker

    def get(self, doc_id):
        authorized = self.get_secure_cookie('authorized')

        if not authorized:
            self.set_status(400)
            self.write('Not authorized')
            return

        session = self.__SessionMaker()

        doc = session.query(DocModel).filter(DocModel.id == doc_id).one()

        org_results = session.query(DocModel.source_org).group_by(
                DocModel.source_org).order_by(DocModel.source_org).all()

        org_names = [each[0] for each in org_results]

        session.close()

        # Make sure dates are in the form understood by JavaScript
        if doc.date_requested:
            doc.date_requested = doc.date_requested.strftime('%m/%d/%Y')

        if doc.date_received:
            doc.date_received = doc.date_received.strftime('%m/%d/%Y')

        self.render(
            'edit.html', region=self.__region, org_names=org_names,
            google_analytics_id=self.__google_analytics_id,
            authorized=authorized, doc=doc
            )

    def post(self, doc_id):
        session = self.__SessionMaker()
        doc = session.query(DocModel).filter(DocModel.id == doc_id).one()

        for each_property in ['doc_title', 'doc_description', 'source_org',
            'tracking_number', 'date_requested', 'date_received',
            'uploader_name', 'uploader_email']:

            value = self.get_argument(each_property)

            if each_property.startswith('date_'):
                if value:
                    value = datetime.strptime(value, '%m/%d/%Y').date()
                else:
                    value = None

            setattr(doc, each_property, value)

        session.add(doc)
        session.commit()

        self.set_cookie('notification', quote('Document updated; thanks!'))
        self.redirect('/')


class DeleteHandler(RequestHandler):

    def initialize(self, SessionMaker, stored_docs_path, s3_bucket_name):
        self.__SessionMaker = SessionMaker
        self.__stored_docs_path = stored_docs_path
        self.__s3_bucket_name = s3_bucket_name

    def post(self):
        # Make sure we are authorized
        authorized = self.get_secure_cookie('authorized')

        if not authorized:
            self.set_status(400)
            self.write('Not authorized')
            return

        body_dict = json.loads(self.request.body)
        doc_id = body_dict.get('doc_id')

        if not doc_id:
            self.set_status(401)
            self.write('Bad request, no doc_id')
            return

        # If S3 is configured, check if we should remove files
        if self.__s3_bucket_name:
            s3_client = boto3.client('s3')
            objs = s3_client.list_objects(
                Bucket=self.__s3_bucket_name,
                Prefix='file/{}/'.format(doc_id),
            )

            objects = [{'Key': each['Key']} for each in objs.get('Contents', [])]
            if objects:
                s3_client.delete_objects(
                    Bucket=self.__s3_bucket_name,
                    Delete={'Objects': objects},
                )

        # Check if we should remove files from on-disk
        doc_folder = os.path.join(self.__stored_docs_path, str(doc_id))
        try:
            os.listdir(doc_folder)
            shutil.rmtree(doc_folder)
        except FileNotFoundError:
            pass

        # Remove metadata
        session = self.__SessionMaker()
        doc = session.query(DocModel).filter(DocModel.id == doc_id).one()
        session.delete(doc)
        session.commit()

        self.set_cookie(
            'notification',
            quote('Deleted document: {}'.format(doc.doc_title)),
            )

        self.write({'success': True})


class OrgHandler(RequestHandler):

    def initialize(self, region, google_analytics_id, SessionMaker):
        self.__region = region
        self.__google_analytics_id = google_analytics_id
        self.__SessionMaker = SessionMaker

    def get(self):
        authorized = self.get_secure_cookie('authorized')

        session = self.__SessionMaker()

        org_results = session.query(
            DocModel.source_org, func.count(DocModel.source_org)).group_by(
                DocModel.source_org).order_by(DocModel.source_org).all()

        session.close()

        self.render(
            'org.html', region=self.__region,
            google_analytics_id=self.__google_analytics_id,
            authorized=authorized,
            org_results=org_results,
            urlencode=urlencode
            )


class SubmitterHandler(RequestHandler):

    def initialize(self, region, google_analytics_id, SessionMaker):
        self.__region = region
        self.__google_analytics_id = google_analytics_id
        self.__SessionMaker = SessionMaker

    def get(self):
        authorized = self.get_secure_cookie('authorized')

        session = self.__SessionMaker()

        submitter_results = session.query(
            DocModel.uploader_name, func.count(DocModel.uploader_name)).group_by(
                DocModel.uploader_name).order_by(DocModel.uploader_name).all()

        session.close()

        self.render(
            'submitter.html', region=self.__region,
            google_analytics_id=self.__google_analytics_id,
            authorized=authorized,
            submitter_results=submitter_results,
            urlencode=urlencode
            )


class AuthHandler(RequestHandler):

    def initialize(self, password):
        self.__password = password

    def get(self):
        # First, check if this is a log out request
        log_out = self.get_argument('log_out', default=None)

        if log_out:
            self.clear_cookie('authorized')
            self.set_cookie('notification', quote('You have signed out'))
            self.redirect('/')
            return

        # If not, make sure basic auth is submitted
        auth_header = self.request.headers.get('Authorization')
        if auth_header:
            auth_header = auth_header.encode('utf8')

        if not auth_header:
            self.set_header('WWW-Authenticate', 'Basic realm=/auth/')
            self.set_status(401)
            return

        else:
            # We have basic auth info; check it
            auth_decoded = base64.decodestring(auth_header[6:]).decode('utf8')
            username, password = auth_decoded.split(':', 2)

            if password == self.__password:
                self.set_secure_cookie('authorized', 'true')
                self.set_cookie('notification', quote('You have signed in succesfully!'))
                self.redirect('/')

            else:
                self.set_header('WWW-Authenticate', 'Basic realm=/auth/')
                self.set_status(401)
                return


class RecentFeedHander(RequestHandler):

    def initialize(self, region, SessionMaker):
        self.__region = region
        self.__SessionMaker = SessionMaker

    @staticmethod
    def _date_to_tzaware_datetime(the_date):
        """
        Given a datetime.date, returns a datetime.date time of noon at that
        date in the local timezone.
        """
        return datetime.combine(
                the_date,
                time(12)
            ).replace(tzinfo=tz.tzlocal())

    def get(self):
        session = self.__SessionMaker()
        recent_docs = session.query(DocModel).order_by(desc('date_uploaded')).limit(15)
        session.close()

        site_url = '{:s}://{:s}'.format(self.request.protocol, self.request.host)
        feed_url = '{:s}{:s}'.format(site_url, self.request.uri)

        fg = FeedGenerator()
        fg.title('{:s} Government Document Repository'
                 .format(self.__region))
        fg.description('Recent uploads to the {:s} Government Document Repository'
                       .format(self.__region))
        fg.updated(
            RecentFeedHander._date_to_tzaware_datetime(recent_docs[0].date_uploaded))
        fg.link(href=feed_url, rel='self')
        fg.link(href=site_url, rel='alternate')

        for doc in recent_docs:
            fe = fg.add_entry()
            entry_link = '{:s}/view/{:d}'.format(site_url, doc.id)
            fe.id(entry_link)
            fe.link(href=entry_link, rel='alternate')
            fe.published(RecentFeedHander._date_to_tzaware_datetime(doc.date_uploaded))
            fe.title(doc.doc_title)
            fe.description('{:s}<br /><b>Source Organization:</b> {:s}'.format(
                bleach.clean(markdown.markdown(doc.doc_description), tags=BLEACH_ALLOWED_TAGS),
                doc.source_org or "-"
            ))

        self.set_header('Content-Type', 'application/rss+xml')
        self.finish(fg.rss_str(pretty=True))


Base = declarative_base()


class DocModel(Base):
    __tablename__ = 'docs'

    id = Column(Integer, primary_key=True)
    doc_title = Column(String)
    doc_description = Column(String)
    source_org = Column(String)
    tracking_number = Column(String, nullable=True)
    date_requested = Column(Date, nullable=True)
    date_received = Column(Date, nullable=True)
    uploader_name = Column(String)
    uploader_email = Column(String)
    date_uploaded = Column(Date)


class LegacyFile(Base):
    __tablename__ = 'legacy_file'

    id = Column(Integer, primary_key=True)
    doc_id = Column(Integer)
    filename = Column(String)


if __name__ == '__main__':
    current_directory = os.path.dirname(__file__)
    static_path = os.path.join(current_directory, 'static')
    template_path = os.path.join(current_directory, 'templates')

    storage_path = os.path.join(current_directory, 'storage')
    stored_docs_path = os.path.join(storage_path, 'docs')
    db_path = os.path.join(storage_path, 'db')

    for each_path in [storage_path, stored_docs_path, db_path]:
        if not os.path.exists(each_path):
            os.mkdir(each_path)

        if not os.access(each_path, os.W_OK):
            print('')
            print('Error: unable to write to {0}'.format(each_path))
            print('')
            print('Please update your filesystem permissions and try again')
            print('')
            sys.exit(-1)

    engine = create_engine('sqlite:///storage/db/sqlite.db')
    Base.metadata.create_all(engine)
    SessionMaker = sessionmaker(engine)

    if not os.path.exists('settings.yml'):
        print('')
        print('Error: application needs settings.yml file to run')
        print('')
        print('For details see settings.sample.yml or README.rst')
        print('')
        sys.exit(-1)

    # try using yaml.safe_load, in case we're using PyYAML 5.1+:
    # https://github.com/yaml/pyyaml/wiki/PyYAML-yaml.load(input)-Deprecation
    # if it fails, fallback to pre-5.1 yaml.load call
    with open('settings.yml', 'r') as fd:
        try:
            settings = yaml.safe_load(fd)
        except AttributeError:
            settings = yaml.load(fd)

    google_analytics_id = settings.get('google_analytics_id', None)
    max_file_size = settings.get('max_file_size', 104857600)

    loggers = ['tornado.access', 'tornado.application', 'tornado.general']

    for each_logger in loggers:
        logger = logging.getLogger(each_logger)
        logger.setLevel(logging.DEBUG)

        handler = logging.StreamHandler()
        handler.setFormatter(
            logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))

        logger.addHandler(handler)

    app = Application([
        (r'/static/(.*)', StaticFileHandler, {'path': static_path}),

        (r'/', IndexHandler, dict(
            region=settings['region'],
            google_analytics_id=google_analytics_id,
            SessionMaker=SessionMaker
            )),

        (r'/rss/recent', RecentFeedHander, dict(
            region=settings['region'],
            SessionMaker=SessionMaker
            )),

        (r'/add', AddHandler, dict(
            region=settings['region'],
            google_analytics_id=google_analytics_id,
            SessionMaker=SessionMaker,
            stored_docs_path=stored_docs_path,
            s3_bucket_name=settings.get('s3_bucket_name'),
            )),

        (r'/search', SearchHandler, dict(
            region=settings['region'],
            google_analytics_id=google_analytics_id,
            SessionMaker=SessionMaker
            )),

        (r'/doc/([0-9]+)/', LegacyHandler),

        (r'/doc/([0-9]+)/view/([0-9]+)', LegacyFileHandler, dict(
            SessionMaker=SessionMaker
            )),

        (r'/view/([0-9]+)', ViewHandler, dict(
            region=settings['region'],
            google_analytics_id=google_analytics_id,
            SessionMaker=SessionMaker,
            stored_docs_path=stored_docs_path,
            s3_bucket_name=settings.get('s3_bucket_name'),
            )),

        (r'/view/([0-9]+)/(.*)', ViewHandler, dict(
            region=settings['region'],
            google_analytics_id=google_analytics_id,
            SessionMaker=SessionMaker,
            stored_docs_path=stored_docs_path
            )),

        (r'/file/([0-9]+)/(.*)', DownloadHandler, dict(
            stored_docs_path=stored_docs_path,
            s3_bucket_name=settings.get('s3_bucket_name'),
            )),

        (r'/edit/([0-9]+)', EditHandler, dict(
            region=settings['region'],
            google_analytics_id=google_analytics_id,
            SessionMaker=SessionMaker
            )),

        (r'/delete', DeleteHandler, dict(
            SessionMaker=SessionMaker,
            stored_docs_path=stored_docs_path,
            s3_bucket_name=settings.get('s3_bucket_name'),
            )),

        (r'/orgs', OrgHandler, dict(
            region=settings['region'],
            google_analytics_id=google_analytics_id,
            SessionMaker=SessionMaker
            )),

        (r'/submitters', SubmitterHandler, dict(
            region=settings['region'],
            google_analytics_id=google_analytics_id,
            SessionMaker=SessionMaker
            )),

        (r'/auth/', AuthHandler, dict(
            password=settings['password']
            ))

        ],

        template_path=template_path,
        cookie_secret=settings['cookie_secret'],
        xsrf_cookies=True,
        debug=settings.get('debug', False),
        )

    server = HTTPServer(app, max_buffer_size=max_file_size)
    server.listen(8000)

    IOLoop.current().start()
